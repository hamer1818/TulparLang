"""
============================================
TULPAR vs PYTHON vs C vs JavaScript vs PHP
       PERFORMANS KARÅžILAÅžTIRMASI
Her dil kendi sÃ¼re Ã¶lÃ§Ã¼mÃ¼nÃ¼ yapar
============================================
"""
import subprocess
import time
import os
import re
import shutil
import platform

# ============================================
# Python Benchmark FonksiyonlarÄ±
# ============================================
import sys

# Increase recursion depth for deep recursion tests
sys.setrecursionlimit(5000)

def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)

def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)

def ackermann(m, n):
    if m == 0:
        return n + 1
    if n == 0:
        return ackermann(m - 1, 1)
    return ackermann(m - 1, ackermann(m, n - 1))

def tak(x, y, z):
    if y < x:
        return tak(tak(x - 1, y, z), tak(y - 1, z, x), tak(z - 1, x, y))
    return z

def loop_test(iterations):
    sum_val = 0
    for i in range(iterations):
        sum_val += i
    return sum_val

def sieve(n):
    flags = [0] * (n + 1)
    count = 0
    for i in range(2, n + 1):
        if flags[i] == 0:
            count += 1
            for k in range(i * i, n + 1, i):
                flags[k] = 1
    return count

def bubble_sort(size):
    arr = []
    seed = 12345
    for i in range(size):
        seed = (seed * 1103515245 + 12345) % 32768
        arr.append(seed)
    
    for i in range(size):
        for j in range(size - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
    
    # Compute checksum of ALL elements
    checksum = sum(arr)
    return checksum

def string_concat(iterations):
    result = ""
    for i in range(iterations):
        result = result + "a"
    return len(result)

def array_memory(size):
    arr = []
    for i in range(size):
        arr.append(i)
    sum_val = 0
    for i in range(size):
        sum_val = sum_val + arr[i]
    return sum_val

def string_allocation(iterations):
    result = ""
    for i in range(iterations):
        result = result + "Merhaba Dunya "
    return len(result)

def json_build_test(size):
    data = []
    for i in range(size):
        data.append({
            'id': i,
            'name': 'Item',
            'value': i * 10,
            'active': True
        })
    sum_val = 0
    for i in range(size):
        sum_val += data[i]['value']
    return sum_val

def run_python_benchmarks():
    """Python benchmark testlerini Ã§alÄ±ÅŸtÄ±r"""
    results = {}
    
    start = time.perf_counter()
    fib_result = fibonacci(30)
    end = time.perf_counter()
    results['fibonacci'] = {'result': fib_result, 'time': (end - start) * 1000}
    
    start = time.perf_counter()
    fact_result = factorial(20)
    end = time.perf_counter()
    results['factorial'] = {'result': fact_result, 'time': (end - start) * 1000}
    
    start = time.perf_counter()
    ack_result = ackermann(3, 8)
    end = time.perf_counter()
    results['ackermann'] = {'result': ack_result, 'time': (end - start) * 1000}

    start = time.perf_counter()
    tak_result = tak(18, 12, 6)
    end = time.perf_counter()
    results['tak'] = {'result': tak_result, 'time': (end - start) * 1000}

    start = time.perf_counter()
    loop_result = loop_test(1000000)
    end = time.perf_counter()
    results['loop'] = {'result': loop_result, 'time': (end - start) * 1000}
    
    start = time.perf_counter()
    sieve_result = sieve(10000)
    end = time.perf_counter()
    results['sieve'] = {'result': sieve_result, 'time': (end - start) * 1000}

    start = time.perf_counter()
    bubble_result = bubble_sort(100)
    end = time.perf_counter()
    results['bubble'] = {'result': bubble_result, 'time': (end - start) * 1000}
    
    start = time.perf_counter()
    str_result = string_concat(1000)
    end = time.perf_counter()
    results['stringconcat'] = {'result': str_result, 'time': (end - start) * 1000}
    
    start = time.perf_counter()
    arr_result = array_memory(10000)
    end = time.perf_counter()
    results['arraymemory'] = {'result': arr_result, 'time': (end - start) * 1000}
    
    start = time.perf_counter()
    str_alloc_result = string_allocation(1000)
    end = time.perf_counter()
    results['stringalloc'] = {'result': str_alloc_result, 'time': (end - start) * 1000}
    
    start = time.perf_counter()
    json_result = json_build_test(1000)
    end = time.perf_counter()
    results['jsonbuild'] = {'result': json_result, 'time': (end - start) * 1000}
    
    return results


def parse_benchmark_output(output):
    """Benchmark Ã§Ä±ktÄ±sÄ±ndan sÃ¼releri parse et"""
    results = {}
    
    # Test sonuÃ§larÄ±nÄ± parse et - multiline Ã§Ä±ktÄ±larÄ± da destekle
    # Tulpar Ã§Ä±ktÄ±sÄ±: "Sure:\n18.6421\n ms" ÅŸeklinde olabilir
    test_patterns = [
        ('fibonacci', r'Test 1:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
        ('factorial', r'Test 2:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
        ('ackermann', r'Test 3:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
        ('tak', r'Test 4:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
        ('loop', r'Test 5:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
        ('sieve', r'Test 6:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
        ('bubble', r'Test 7:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
        ('stringconcat', r'Test 8:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
        ('arraymemory', r'Test 9:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
        ('stringalloc', r'Test 10:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
        ('jsonbuild', r'Test 11:.*?Sure:\s*([\d.eE+-]+)\s*(ms|Âµs|us)'),
    ]
    
    for name, pattern in test_patterns:
        match = re.search(pattern, output, re.DOTALL | re.IGNORECASE)
        if match:
            val = float(match.group(1))
            unit = match.group(2).lower()
            if unit in ['Âµs', 'us']:
                val /= 1000.0  # Âµs -> ms
            results[name] = val
    
    # Toplam sÃ¼reyi parse et
    total_match = re.search(r'TOPLAM SURE:\s*([\d.]+)\s*ms', output, re.IGNORECASE)
    if total_match:
        results['total'] = float(total_match.group(1))
    else:
        results['total'] = sum(results.values()) if results else 0
    
    return results

# ... (rest of the file) ...

    test_names = {
        'fibonacci': 'Fibonacci(30)',
        'factorial': 'Factorial(20)',
        'ackermann': 'Ackermann(3, 8)',
        'tak': 'Tak(18, 12, 6)',
        'loop': 'Loop 1M',
        'sieve': 'Sieve(10K)',
        'bubble': 'BubbleSort(1K)',
        'stringconcat': 'StringConcat(1K)',
        'arraymemory': 'ArrayMemory(10K)',
        'stringalloc': 'StringAlloc(1K)',
        'jsonbuild': 'JSONBuild(1K)'
    }

def run_tulpar_benchmark():
    """Tulpar benchmark testini Ã§alÄ±ÅŸtÄ±r"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.join(script_dir, '..')
    
    # Try multiple possible Tulpar executable locations
    if platform.system() == 'Windows':
        possible_paths = [
            os.path.join(project_dir, 'build-win', 'tulpar.exe'),
            os.path.join(project_dir, 'build', 'tulpar.exe'),
            os.path.join(project_dir, 'tulpar.exe'),
        ]
    else:
        possible_paths = [
            os.path.join(project_dir, 'build-linux', 'tulpar'),
            os.path.join(project_dir, 'build', 'tulpar'),
            os.path.join(project_dir, 'tulpar'),
        ]
    
    tulpar_exe = None
    for path in possible_paths:
        if os.path.exists(path):
            tulpar_exe = path
            break
        
    benchmark_file = os.path.join(script_dir, 'benchmark.tpr')
    output_bin = os.path.join(script_dir, 'benchmark_tulpar')
    if platform.system() == 'Windows':
        output_bin += '.exe'
    
    if not tulpar_exe:
        tried_paths = '\n    '.join(possible_paths)
        return {'error': f'Tulpar bulunamadÄ±. Aranan yollar:\n    {tried_paths}'}
    
    try:
        # Try AOT with the compatible benchmark first
        aot_benchmark = os.path.join(script_dir, 'benchmark_aot.tpr')
        
        if os.path.exists(aot_benchmark):
            # Use AOT-compatible benchmark
            compile_cmd = [tulpar_exe, '--aot', aot_benchmark, output_bin]
            compile_result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=60, cwd=project_dir)
            
            # Handle double .exe issue on Windows
            actual_bin = output_bin + '.exe' if platform.system() == 'Windows' and os.path.exists(output_bin + '.exe') else output_bin
            
            if compile_result.returncode == 0 and os.path.exists(actual_bin):
                result = subprocess.run([actual_bin], capture_output=True, text=True, timeout=300, cwd=project_dir)
                
                # Cleanup
                try:
                    os.remove(actual_bin)
                    for ext in ['.ll', '.o', '.exe.ll', '.exe.o']:
                        cleanup_file = output_bin + ext
                        if os.path.exists(cleanup_file):
                            os.remove(cleanup_file)
                except:
                    pass
                
                return {'results': parse_benchmark_output(result.stdout), 'output': result.stdout}
        
        # Fallback to VM mode for full benchmark compatibility
        run_cmd = [tulpar_exe, benchmark_file]
        result = subprocess.run(run_cmd, capture_output=True, text=True, timeout=300, cwd=project_dir)
        return {'results': parse_benchmark_output(result.stdout), 'output': result.stdout}
        
    except subprocess.TimeoutExpired:
        return {'error': 'Timeout'}
    except Exception as e:
        return {'error': str(e)}


def run_c_benchmark():
    """C benchmark testini Ã§alÄ±ÅŸtÄ±r"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    c_source = os.path.join(script_dir, 'benchmark.c')
    c_exe = os.path.join(script_dir, 'benchmark_c.exe' if platform.system() == 'Windows' else 'benchmark_c')
    
    if not os.path.exists(c_source):
        return {'error': 'benchmark.c bulunamadÄ±'}
    
    # GCC kontrolÃ¼
    gcc = shutil.which('gcc')
    if not gcc:
        return {'error': 'GCC bulunamadÄ±. C derleyicisi yÃ¼klÃ¼ deÄŸil.'}
    
    try:
        # Derle
        compile_result = subprocess.run(
            ['gcc', '-O2', c_source, '-o', c_exe],
            capture_output=True, text=True, timeout=60
        )
        if compile_result.returncode != 0:
            return {'error': f'Derleme hatasÄ±: {compile_result.stderr}'}
        
        # Ã‡alÄ±ÅŸtÄ±r
        result = subprocess.run([c_exe], capture_output=True, text=True, timeout=300)
        
        # Temizle
        if os.path.exists(c_exe):
            os.remove(c_exe)
        
        return {'results': parse_benchmark_output(result.stdout), 'output': result.stdout}
    except subprocess.TimeoutExpired:
        return {'error': 'Timeout'}
    except Exception as e:
        return {'error': str(e)}


def run_javascript_benchmark():
    """JavaScript (Node.js) benchmark testini Ã§alÄ±ÅŸtÄ±r"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    js_file = os.path.join(script_dir, 'benchmark.js')
    
    if not os.path.exists(js_file):
        return {'error': 'benchmark.js bulunamadÄ±'}
    
    # Node.js kontrolÃ¼
    node = shutil.which('node')
    if not node:
        return {'error': 'Node.js bulunamadÄ±. Node.js yÃ¼klÃ¼ deÄŸil.'}
    
    try:
        result = subprocess.run(['node', js_file], capture_output=True, text=True, timeout=300)
        return {'results': parse_benchmark_output(result.stdout), 'output': result.stdout}
    except subprocess.TimeoutExpired:
        return {'error': 'Timeout'}
    except Exception as e:
        return {'error': str(e)}


def run_php_benchmark():
    """PHP benchmark testini Ã§alÄ±ÅŸtÄ±r"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    php_file = os.path.join(script_dir, 'benchmark.php')
    
    if not os.path.exists(php_file):
        return {'error': 'benchmark.php bulunamadÄ±'}
    
    # PHP kontrolÃ¼
    php = shutil.which('php')
    if not php:
        return {'error': 'PHP bulunamadÄ±. PHP CLI yÃ¼klÃ¼ deÄŸil.'}
    
    try:
        result = subprocess.run(['php', php_file], capture_output=True, text=True, timeout=300)
        return {'results': parse_benchmark_output(result.stdout), 'output': result.stdout}
    except subprocess.TimeoutExpired:
        return {'error': 'Timeout'}
    except Exception as e:
        return {'error': str(e)}


def format_time(ms):
    if ms is None:
        return "N/A"
    elif ms == 0:
        return "<0.01 Âµs"  # Too fast to measure
    elif ms < 1:
        return f"{ms * 1000:.2f} Âµs"
    elif ms < 1000:
        return f"{ms:.3f} ms"
    else:
        return f"{ms / 1000:.3f} s"


def main():
    print("=" * 80)
    print("       ðŸ MULTI-LANGUAGE PERFORMANS KARÅžILAÅžTIRMASI ðŸ")
    print("            Tulpar vs Python vs C vs JavaScript vs PHP")
    print("=" * 80)
    print(f"Tarih: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Platform: {platform.system()} {platform.release()}")
    print()
    
    all_results = {}
    
    # ============================================
    # Benchmark'larÄ± Ã‡alÄ±ÅŸtÄ±r
    # ============================================
    
    # 1. C
    print("âš¡ C Benchmark Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    c_data = run_c_benchmark()
    if 'results' in c_data:
        all_results['C'] = c_data['results']
        print("âœ… C Benchmark TamamlandÄ±!")
    else:
        print(f"âš ï¸  C Benchmark AtlandÄ±: {c_data.get('error', 'Bilinmeyen hata')}")
        all_results['C'] = {}
    
    # 2. JavaScript
    print("ðŸŸ¨ JavaScript Benchmark Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    js_data = run_javascript_benchmark()
    if 'results' in js_data:
        all_results['JavaScript'] = js_data['results']
        print("âœ… JavaScript Benchmark TamamlandÄ±!")
    else:
        print(f"âš ï¸  JavaScript Benchmark AtlandÄ±: {js_data.get('error', 'Bilinmeyen hata')}")
        all_results['JavaScript'] = {}
    
    # 3. Python
    print("ðŸ Python Benchmark Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    python_results = run_python_benchmarks()
    python_total = sum([r['time'] for r in python_results.values()])
    all_results['Python'] = {k: v['time'] for k, v in python_results.items()}
    all_results['Python']['total'] = python_total
    print("âœ… Python Benchmark TamamlandÄ±!")
    
    # 4. PHP
    print("ðŸ˜ PHP Benchmark Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    php_data = run_php_benchmark()
    if 'results' in php_data:
        all_results['PHP'] = php_data['results']
        print("âœ… PHP Benchmark TamamlandÄ±!")
    else:
        print(f"âš ï¸  PHP Benchmark AtlandÄ±: {php_data.get('error', 'Bilinmeyen hata')}")
        all_results['PHP'] = {}
    
    # 5. Tulpar
    print("ðŸŽ Tulpar Benchmark Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    tulpar_data = run_tulpar_benchmark()
    if 'results' in tulpar_data:
        all_results['Tulpar'] = tulpar_data['results']
        print("âœ… Tulpar Benchmark TamamlandÄ±!")
    else:
        print(f"âš ï¸  Tulpar Benchmark AtlandÄ±: {tulpar_data.get('error', 'Bilinmeyen hata')}")
        all_results['Tulpar'] = {}
    
    print()
    
    # ============================================
    # DetaylÄ± SonuÃ§lar
    # ============================================
    print("=" * 80)
    print("                           DETAYLI SONUÃ‡LAR")
    print("=" * 80)
    print()
    
    test_names = {
        'fibonacci': 'Fibonacci(30)',
        'factorial': 'Factorial(20)',
        'ackermann': 'Ackermann(3, 8)',
        'tak': 'Tak(18, 12, 6)',
        'loop': 'Loop 1M',
        'sieve': 'Sieve(10K)',
        'bubble': 'BubbleSort(1K)',
        'stringconcat': 'StringConcat(1K)',
        'arraymemory': 'ArrayMemory(10K)',
        'stringalloc': 'StringAlloc(1K)',
        'jsonbuild': 'JSONBuild(1K)'
    }
    
    languages = ['C', 'JavaScript', 'Python', 'PHP', 'Tulpar']
    
    # Header
    header = f"  {'Test':<20}"
    for lang in languages:
        header += f" {lang:<14}"
    print(header)
    print("  " + "-" * (20 + 15 * len(languages)))
    
    # Test SonuÃ§larÄ±
    for key, name in test_names.items():
        row = f"  {name:<20}"
        for lang in languages:
            val = all_results.get(lang, {}).get(key, 0)
            row += f" {format_time(val):<14}"
        print(row)
    
    print()
    
    # ============================================
    # Toplam KarÅŸÄ±laÅŸtÄ±rma
    # ============================================
    print("=" * 80)
    print("                         TOPLAM SÃœRE KARÅžILAÅžTIRMASI")
    print("=" * 80)
    print()
    
    # SÄ±ralama iÃ§in toplam sÃ¼releri al
    totals = []
    for lang in languages:
        total = all_results.get(lang, {}).get('total', 0)
        if total > 0:
            totals.append((lang, total))
    
    # SÄ±rala (en hÄ±zlÄ±dan en yavaÅŸa)
    totals.sort(key=lambda x: x[1])
    
    if totals:
        fastest_lang, fastest_time = totals[0]
        
        print(f"  {'SÄ±ra':<6} {'Dil':<15} {'Toplam SÃ¼re':<18} {'KarÅŸÄ±laÅŸtÄ±rma':<25}")
        print("  " + "-" * 70)
        
        for i, (lang, total) in enumerate(totals, 1):
            if i == 1:
                comparison = "ðŸ† EN HIZLI"
            else:
                ratio = total / fastest_time
                comparison = f"ðŸ“‰ {ratio:.2f}x yavaÅŸ"
            
            medal = "ðŸ¥‡" if i == 1 else "ðŸ¥ˆ" if i == 2 else "ðŸ¥‰" if i == 3 else "  "
            print(f"  {medal} {i:<3} {lang:<15} {format_time(total):<18} {comparison:<25}")
    
    print()
    
    # ============================================
    # Ã–zet
    # ============================================
    print("=" * 80)
    print("                              ðŸ“Š Ã–ZET")
    print("=" * 80)
    print()
    
    if totals:
        fastest_lang, fastest_time = totals[0]
        
        # Tulpar'Ä±n konumu
        tulpar_total = all_results.get('Tulpar', {}).get('total', 0)
        if tulpar_total > 0:
            tulpar_rank = next((i for i, (l, _) in enumerate(totals, 1) if l == 'Tulpar'), 0)
            ratio_to_fastest = tulpar_total / fastest_time if fastest_time > 0 else 0
            
            print(f"  ðŸŽ Tulpar SÄ±ralamasÄ±: {tulpar_rank}/{len(totals)}")
            print(f"  â±ï¸  Tulpar Toplam SÃ¼re: {format_time(tulpar_total)}")
            print(f"  ðŸ“ˆ En hÄ±zlÄ±ya ({fastest_lang}) gÃ¶re: {ratio_to_fastest:.2f}x yavaÅŸ")
            
            # Python ile karÅŸÄ±laÅŸtÄ±r
            python_total = all_results.get('Python', {}).get('total', 0)
            if python_total > 0:
                if tulpar_total < python_total:
                    print(f"  ðŸŽ‰ Python'dan {python_total/tulpar_total:.2f}x HIZLI!")
                else:
                    print(f"  ðŸ“‰ Python'dan {tulpar_total/python_total:.2f}x yavaÅŸ")
    
    print()
    print("=" * 80)
    
    # ============================================
    # Dosyaya Kaydet
    # ============================================
    output_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'benchmark_results.txt')
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("MULTI-LANGUAGE PERFORMANS KARÅžILAÅžTIRMASI\n")
        f.write("Tulpar vs Python vs C vs JavaScript vs PHP\n")
        f.write("=" * 80 + "\n")
        f.write(f"Tarih: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Platform: {platform.system()} {platform.release()}\n\n")
        
        f.write("-" * 80 + "\n")
        f.write("TOPLAM SÃœRELER (SÄ±ralÄ±)\n")
        f.write("-" * 80 + "\n\n")
        
        for i, (lang, total) in enumerate(totals, 1):
            f.write(f"{i}. {lang}: {format_time(total)}\n")
        
        f.write("\n" + "=" * 80 + "\n")
    
    print(f"\n  ðŸ“ SonuÃ§lar kaydedildi: {output_file}")


if __name__ == "__main__":
    main()
